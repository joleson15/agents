{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896f56bc",
   "metadata": {},
   "source": [
    "# Agent 2 Agent Protocol (A2A) - a thread:\n",
    "\n",
    "### What is A2A?\n",
    "A2A is a protocol to allow AI agents to communicate autonomously amongst themselves in a standardized way. It was introduced by Google in April 2025 and donated to the Linux Foundation for more widespread adoption. As the sophistication of AI systems grows, so has the ability to automate daily tasks. However, scaling multi-agent systems has been a major bottleneck for many engineers. Frameworks like langgraph seek to solve this problem, but the framework is not very scalable and the api's don't always fit the needs of developers, can become outdated, and may introduce breaking changes...hence the need for custom agents, and modularity. A2A is an effort by Google to standardize inter-agent communication and allow developers to integrate the capabilities of various frameworks. \n",
    "\n",
    "The probabalistic origins of machine learning and AI models cause a significant amount of non-determinism in the outputs, on the contrary to traditional algorithms that provide a deterministic output (i.e. given the same input, it always produces the same output). Because of this, we can't always predict the output of a machine learning algorithm. So maybe it is obvious that in order to allow these systems to share information, the outputs must be wrapped in some metadata to provide some scaffolding for a message format, even though the messages themselves are always unique. \n",
    "\n",
    "\n",
    "See also MCP, a protocol introduced by Anthropic to provide a standard of communication between AI models and tools, data, prompts, and other resources. While MCP allows the AI \"agent\" external functionality like querying a database, searching the internet, or executing a workflow, A2A allows multiple specialized AI agents to communicate with each other and complete complex tasks by utilizing their variety of skills. If the A2A protocol reaches a critical mass, it will hopefully become the de-facto standard of communication for the new era of intelligent systems\n",
    "\n",
    "Example: Google Search Agent, Database Query agent, summarizer agent, analysis agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec7194",
   "metadata": {},
   "source": [
    "### The Agent\n",
    "\n",
    "So what is an Agent? While the A2A protocol seems like it has been standardized, the definition of \"agent\" certainly has not. Langchain and Microsoft define an [agent](https://langchain-ai.github.io/langgraph/agents/overview/) as \"an LLM + tools + a prompt\". Google has more broadly stated that an AI agent is a software system that uses AI (not just LLMs) to proactively pursue goals and complete tasks. This allows us to expand the scope of agents to include more specialized machine learning algorithms, although the implementation of Google's agent development kit ([ADK](https://google.github.io/adk-docs/)) indicates that they really fall into the first camp. For now, I will focus on the \"LLM + tools + prompt\" definition, but stay tuned for more exploration into nuanced algorithms in the future. \n",
    "\n",
    "Let's take a look at our first \"agent\"...\n",
    "\n",
    "**Prerequisites**:\n",
    "- Pyton 3.11+\n",
    "- [Gemini API Key](https://aistudio.google.com/apikey)l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aecd0651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# if not os.getenv('GOOGLE_GEMINI_API_KEY'):\n",
    "#     os.environ['GOOGLE_GEMINI_API_KEY'] = 'AIzaSyCXbHHWO-vCDl31AU4EPQTIp5-pvl_QTLA'\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cde5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "from google.adk.tools import google_search\n",
    "\n",
    "my_first_agent = Agent(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    name=\"My_first_agent\",\n",
    "    description=\"A simple agent that can call a google search\",\n",
    "    instruction=\"You are a helpful google search agent. Conduct a search when you determine it is necessary to do so.\",\n",
    "    tools=[google_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cccf6",
   "metadata": {},
   "source": [
    "As you can see, there is not much effort to declare your first agent. Google's ADK abstracts away a lot of the headache, it's details are out of scope for this work, but will be covered in a future blog post. Each of the supplied parameters are pretty self-descriptive:\n",
    "- Model: the large language model used as the intelligence engine for your agent\n",
    "- Name: a descriptive name, can only contain letters, numbers and underscores\n",
    "- Description: a helpful description can go a long way, especially once multiple agents are involved in a complex process\n",
    "- Instruction: an optional system prompt that gets passed in as context to the LLM that contains any additional information or instructions\n",
    "- Tools: a list of tools an agent is capable of using. In this case we have only outfitted our agent with the ability to conduct a google search\n",
    "\n",
    "Cool, now we have a primitive agent. So how do we handle messages?\n",
    "\n",
    "### Agent Executor\n",
    "The Agent Executor interface is contained in the a2a library, and provides a wrapper for the Agent to handle requests. The interface contains two main methods: `execute()`, which handles the main execution logic of the Agent's runtime and `cancel()`, which can be called during the execution of a long-running task. \n",
    "\n",
    "```python\n",
    "class AgentExecutor(ABC):\n",
    "    \"\"\"Agent Executor interface.\n",
    "\n",
    "    Implementations of this interface contain the core logic of the agent,\n",
    "    executing tasks based on requests and publishing updates to an event queue.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    async def execute(\n",
    "        self, context: RequestContext, event_queue: EventQueue\n",
    "    ) -> None:\n",
    "        \"\"\"Execute the agent's logic for a given request context.\n",
    "\n",
    "        The agent should read necessary information from the `context` and\n",
    "        publish `Task` or `Message` events, or `TaskStatusUpdateEvent` /\n",
    "        `TaskArtifactUpdateEvent` to the `event_queue`. This method should\n",
    "        return once the agent's execution for this request is complete or\n",
    "        yields control (e.g., enters an input-required state).\n",
    "\n",
    "        Args:\n",
    "            context: The request context containing the message, task ID, etc.\n",
    "            event_queue: The queue to publish events to.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    async def cancel(\n",
    "        self, context: RequestContext, event_queue: EventQueue\n",
    "    ) -> None:\n",
    "        \"\"\"Request the agent to cancel an ongoing task.\n",
    "\n",
    "        The agent should attempt to stop the task identified by the task_id\n",
    "        in the context and publish a `TaskStatusUpdateEvent` with state\n",
    "        `TaskState.canceled` to the `event_queue`.\n",
    "\n",
    "        Args:\n",
    "            context: The request context containing the task ID to cancel.\n",
    "            event_queue: The queue to publish the cancellation status update to.\n",
    "        \"\"\"\n",
    "\n",
    "```\n",
    "For now, we will focus only on event execution and leave cancellation to a future blog. Google provides a nice implementation of `execute()` for a generic ADK agent in their [A2A Samples](https://github.com/a2aproject) repo that I will borrow for this example.\n",
    "\n",
    "\n",
    "The `execute()` method concurrently processes requests by keeping track of them with an `EventQueue`. Requests are presented in the form of a `RequestContext`, which contains the content of the request, along with other metadata. That is enough info for now, we will get into the weeds in future work.\n",
    "\n",
    "A critical component to the successful operation of an Agent is the `session`. A session is a stateful container that allows agents to interact asynchronously. It manages the ongoing interaction between the agents and/or users, context such as memory and state, and a request/response loop to handle communication and execution of actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2a.server.agent_execution import AgentExecutor, RequestContext\n",
    "from a2a.server.events import EventQueue\n",
    "from a2a.server.tasks import TaskUpdater\n",
    "from a2a.utils import new_task, new_agent_text_message\n",
    "from a2a.types import TaskState, TextPart, Part\n",
    "\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.artifacts import InMemoryArtifactService\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.memory import InMemoryMemoryService\n",
    "from google.genai import types\n",
    "\n",
    "class MyAgentExecutor(AgentExecutor):\n",
    "\n",
    "    def __init__(self, agent: Agent, status_message: str = \"Executing task...\", artifact_name: str = \"response\"):\n",
    "        self.agent = agent\n",
    "        self.status_message = status_message\n",
    "        self.artifact_name = artifact_name\n",
    "        self.runner = Runner(\n",
    "            app_name=agent.name,\n",
    "            agent=agent,\n",
    "            artifact_service=InMemoryArtifactService(),\n",
    "            session_service=InMemorySessionService(),\n",
    "            memory_service=InMemoryMemoryService(),\n",
    "        )\n",
    "\n",
    "    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:\n",
    "        query = context.get_user_input()\n",
    "        task = context.current_task or new_task(context.message)\n",
    "        await event_queue.enqueue_event(task)\n",
    "\n",
    "        updater = TaskUpdater(event_queue, task.id, task.contextId)\n",
    "\n",
    "        try:\n",
    "            # Update status with custom message\n",
    "            await updater.update_status(\n",
    "                TaskState.working,\n",
    "                new_agent_text_message(self.status_message, task.contextId, task.id)\n",
    "            )\n",
    "\n",
    "            # Process with ADK agent\n",
    "            session = await self.runner.session_service.create_session(\n",
    "                app_name=self.agent.name,\n",
    "                user_id=\"a2a_user\",\n",
    "                state={},\n",
    "                session_id=task.contextId,\n",
    "            )\n",
    "\n",
    "            content = types.Content(\n",
    "                role='user',\n",
    "                parts=[types.Part.from_text(text=query)]\n",
    "            )\n",
    "\n",
    "            response_text = \"\"\n",
    "            async for event in self.runner.run_async(\n",
    "                user_id=\"a2a_user\",\n",
    "                session_id=session.id,\n",
    "                new_message=content\n",
    "            ):\n",
    "                if event.is_final_response() and event.content and event.content.parts:\n",
    "                    for part in event.content.parts:\n",
    "                        if hasattr(part, 'text') and part.text:\n",
    "                            response_text += part.text + '\\n'\n",
    "                        elif hasattr(part, 'function_call'):\n",
    "                            # Log or handle function calls if needed\n",
    "                            pass  # Function calls are handled internally by ADK\n",
    "\n",
    "            # Add response as artifact with custom name\n",
    "            await updater.add_artifact(\n",
    "                [Part(root=TextPart(text=response_text))],\n",
    "                name=self.artifact_name\n",
    "            )\n",
    "\n",
    "            await updater.complete()\n",
    "\n",
    "        except Exception as e:\n",
    "            await updater.update_status(\n",
    "                TaskState.failed,\n",
    "                new_agent_text_message(f\"Error: {e!s}\", task.contextId, task.id),\n",
    "                final=True\n",
    "            )\n",
    "\n",
    "\n",
    "    def cancel(self, context, event_queue):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01caeb61",
   "metadata": {},
   "source": [
    "Now that we can run an agent, how do we (and more importantly other agents, since this is a blog about A2A after all...) know what its capabilities are?\n",
    "\n",
    "### Agent Card\n",
    "The agent card is a public-facing JSON schema that exposes information and metadata to clients (users and other agents). It is like a user profile, but for AI agents. To define an agent card properly, you need:\n",
    "- `name`\n",
    "- `description`: description of the agent, its skills, and other useful information\n",
    "- `version`\n",
    "- `url`: the web endpoint where we can find our agent\n",
    "- `capabilities`: supported A2A features like streaming or push notifications\n",
    "- `skills`: A pillar of A2A, the skills discovered in the `AgentCard` come in a list of `AgentSkill` objects.\n",
    "\n",
    "For public agents, the A2A project reccomends that the agent card be discoverable at a well-known uri. The standard discovery path is: `https://{server-url}/.well-known/agent.json` This is the discovery method we will use for now, but for more information, see the [A2A discovery](https://a2aproject.github.io/A2A/latest/topics/agent-discovery/#the-role-of-the-agent-card) page.\n",
    "\n",
    "Lets give our agent an `AgentCard`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6881a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2a.types import AgentCard, AgentCapabilities\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=my_first_agent.name,\n",
    "    description=my_first_agent.description,\n",
    "    url=\"http://localhost:9999/\",\n",
    "    version='1.0',\n",
    "    capabilities=AgentCapabilities(\n",
    "        streaming=True\n",
    "    ),\n",
    "    defaultInputModes=[\"text\", \"text/plain\"],\n",
    "    defaultOutputModes=[\"text\", \"text/plain\"],\n",
    "    skills=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce949d",
   "metadata": {},
   "source": [
    "### Agent Skills\n",
    "Agent skills describe specific capabilities the agent has, like searching the web, querying a database, executing an algorithmm...etc. Clients can find out what skills an agent has from the `AgentCard`. It's kind of like the agentic version of a resume. Skills have some attributes to define:\n",
    "- `id`: a unique id\n",
    "- `name`\n",
    "- `description`: more detailed information about the skill's functionality\n",
    "- `tags`: keywords\n",
    "- `examples`: example usage of the skill\n",
    "- `inputModes` and `outputModes`: supported modes for input and output, like text or json\n",
    "\n",
    "Let's go back and define the Google search `AgentSkill` for our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a53a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2a.types import AgentSkill\n",
    "\n",
    "web_search_skill = AgentSkill(\n",
    "    id='google_search',\n",
    "    name='Google Search',\n",
    "    description='Searches the web using the google_search tool',\n",
    "    tags=['web search', 'google', 'search', 'look up']\n",
    ")\n",
    "\n",
    "agent_card.skills.append(web_search_skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a48fa4",
   "metadata": {},
   "source": [
    "Now our agent has advertized the ability to search the web on its agent card. We will take a deep dive into `AgentSkill`s, `AgentCapabilities`, and `AgentCard`s another time, but now lets zoom out a little bit. We have given lots of detail about our agent. Where it lives, what it can do, examples for how to use it... So how do we run it and start testing this stuff out?\n",
    "\n",
    "### Starting the Server\n",
    "\n",
    "A2A follows a client-server architecture, where the client--a user-facing application or agent--initiates a request to other agents acting as servers that handle those requests, similarly to a traditional web browser or API sending requests to a remote web server. They send structured metadata and information over HTTP using [JSON-RPC 2.0](https://www.jsonrpc.org/specification) as the format.\n",
    "\n",
    "With all of the agent functionality defined thus far, let's deploy an `agent` as a `server`. The A2A library provides a nice out-of-the-box app using the [Starlette](https://www.starlette.io/) framework to implement the server endpoints and route JSON-RPC requests. This is perfect for our use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc01fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2a.server.request_handlers import DefaultRequestHandler\n",
    "from a2a.server.tasks import InMemoryTaskStore\n",
    "from a2a.server.apps import A2AStarletteApplication\n",
    "\n",
    "\n",
    "def create_server():\n",
    "\n",
    "    request_handler = DefaultRequestHandler(\n",
    "        agent_executor=MyAgentExecutor(my_first_agent),\n",
    "        task_store=InMemoryTaskStore()\n",
    "    )\n",
    "\n",
    "    return A2AStarletteApplication(\n",
    "        agent_card=agent_card,\n",
    "        http_handler=request_handler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9c2ce",
   "metadata": {},
   "source": [
    "The server application takes a `DefaultRequestHandler` as an argument, which is the engine for all A2A protocol methods, the `AgentExecutor`, the `TaskStore`, the `QueueManager`, and all other JSON-RPC related logic. Lets spin up the server. \n",
    "\n",
    "Since we are using jupyter, we need some fancy event handling so as to not have the server process cause the cell to run forever. We need to run it in a background process using a thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c6332c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is running at http://127.0.0.1:9999\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import threading\n",
    "import time\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def run_agent():\n",
    "\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    \n",
    "    server = create_server()\n",
    "    config = uvicorn.Config(\n",
    "        app=server.build(),\n",
    "        host=\"127.0.0.1\",\n",
    "        port=9999,\n",
    "        log_level=\"error\"\n",
    "    )\n",
    "\n",
    "    uvicorn_server = uvicorn.Server(config)\n",
    "    loop.run_until_complete(uvicorn_server.serve())\n",
    "\n",
    "\n",
    "thread = threading.Thread(target=run_agent, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"Server is running at http://127.0.0.1:9999\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0239c",
   "metadata": {},
   "source": [
    "**Quick note: for now, if we need to stop the server, we have to restart the jupyter kernel.**\n",
    "\n",
    "Great news! We now have our very first agent up and running! So now what? We want to be able to interact with our AI agent and benefit from all of the cool stuff it can do. As I mentioned, A2A communication occurs over HTTP, so lets send a simple http request to our agent. We can start by fetching the `AgentCard` to see what our agent can do.\n",
    "\n",
    "### Fetching the `AgentCard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71387872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"capabilities\": {\n",
      "    \"streaming\": true\n",
      "  },\n",
      "  \"defaultInputModes\": [\n",
      "    \"text\",\n",
      "    \"text/plain\"\n",
      "  ],\n",
      "  \"defaultOutputModes\": [\n",
      "    \"text\",\n",
      "    \"text/plain\"\n",
      "  ],\n",
      "  \"description\": \"A simple agent that can call a google search\",\n",
      "  \"name\": \"My_first_agent\",\n",
      "  \"protocolVersion\": \"0.2.5\",\n",
      "  \"skills\": [\n",
      "    {\n",
      "      \"description\": \"Searches the web using the google_search tool\",\n",
      "      \"id\": \"google_search\",\n",
      "      \"name\": \"Google Search\",\n",
      "      \"tags\": [\n",
      "        \"web search\",\n",
      "        \"google\",\n",
      "        \"search\",\n",
      "        \"look up\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"url\": \"http://localhost:9999/\",\n",
      "  \"version\": \"1.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "\n",
    "async with httpx.AsyncClient() as http_client:\n",
    "    response = await http_client.get(\"http://localhost:9999/.well-known/agent.json\")\n",
    "    agent_card_json = response.json()\n",
    "\n",
    "print(json.dumps(agent_card_json, indent=2)) #json.dumps is completely optional but makes the output more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13bb4c",
   "metadata": {},
   "source": [
    "Amazing! Just as we defined it earlier, the `AgentCard` is showing all of the information about our agent. To interact with our A2A server properly, we need an A2A client. \n",
    "\n",
    "```python\n",
    "from a2a.client import A2AClient\n",
    "\n",
    "my_first_agent_card = AgentCard(**agent_card_json)\n",
    "\n",
    "a2a_client = A2AClient(\n",
    "    httpx_client=http_client,\n",
    "    agent_card=my_first_agent_card,\n",
    "    url=\"http://localhost:9999/\") \n",
    "```\n",
    "\n",
    "`agent_card` and `url` parameters are not both required, but one of them is neeeded to fetch the url of the agent's RPC endpoint. With our client, we can send RPC messages to the A2A server (our agent) by sending an HTTP POST request to the RPC endpoint (the URL). So how do we structure this message and what is it going to look like?\n",
    "\n",
    "### JSON RPC\n",
    "\n",
    "Without diving too deep into the weeds of JSON-RPC 2.0, we can analyze the layout of a simple A2A message, encapsulated in a `JSONRPCRequest` object:\n",
    "- `jsonrpc`: must be `2.0`\n",
    "- `method`: `\"message/send\"`, `\"tasks/get\"` ...\n",
    "- `params`: typically an `object` containing the parameters used for invocation\n",
    "- `id`: unique identifier, not always required if the request does not expect a response.\n",
    "\n",
    "More info [here](https://a2aproject.github.io/A2A/v0.2.5/specification/#611-json-rpc-structures). We will use the `message/send` method, and pass in our first message:\n",
    "\n",
    "```python\n",
    "rpc_request = {\n",
    "    'message': {\n",
    "        'role': 'user',\n",
    "        'parts': [\n",
    "            {'kind': 'text', 'text': 'What are some trending topics in AI right now?'}\n",
    "        ],\n",
    "        'messageId': uuid.uuid4().hex,\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The python sdk for A2A provides lots of useful helper methods that help properly structure our input. Here, we'll create a `SendMessageRequest` and pass in the `rpc_request` as the `params` to our `JSONRPCRequest` object using the `MessageSendParams` method:\n",
    "\n",
    "```python\n",
    "request = SendMessageRequest(\n",
    "        id=str(uuid.uuid4()),\n",
    "        params=MessageSendParams(**rpc_request)\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52493300",
   "metadata": {},
   "source": [
    "##### Optional: run the below cell to view the whole RPC Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19155bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from a2a.types import SendMessageRequest, MessageSendParams\n",
    "\n",
    "rpc_request = {\n",
    "            'message': {\n",
    "                'role': 'user',\n",
    "                'parts': [\n",
    "                    {'kind': 'text', 'text': 'What are some trending topics in AI right now?'}\n",
    "                ],\n",
    "                'messageId': uuid.uuid4().hex,\n",
    "            }\n",
    "    }\n",
    "\n",
    "request = SendMessageRequest(\n",
    "    id=str(uuid.uuid4()),\n",
    "    params=MessageSendParams(**rpc_request)\n",
    ")\n",
    "\n",
    "print(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735180b2",
   "metadata": {},
   "source": [
    "Tying everything together, we can send a properly formatted JSON RPC request to our server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b8eaf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from a2a.types import SendMessageRequest, MessageSendParams\n",
    "from a2a.client import A2AClient\n",
    "\n",
    "timeout_config = httpx.Timeout(\n",
    "            timeout=120.0,\n",
    "            connect=10.0,\n",
    "            read=120.0,\n",
    "            write=10.0,\n",
    "            pool=5.0\n",
    "        )\n",
    "\n",
    "async with httpx.AsyncClient(timeout=timeout_config) as http_client:\n",
    "\n",
    "    response = await http_client.get(\"http://localhost:9999/.well-known/agent.json\")\n",
    "    agent_card_json = response.json()\n",
    "\n",
    "    my_first_agent_card = AgentCard(**agent_card_json)\n",
    "\n",
    "    a2a_client = A2AClient(\n",
    "        httpx_client=http_client,\n",
    "        agent_card=my_first_agent_card,\n",
    "    )\n",
    "\n",
    "    message = \"What are some trending topics in AI right now?\"\n",
    "\n",
    "    rpc_request = {\n",
    "            'message': {\n",
    "                'role': 'user',\n",
    "                'parts': [\n",
    "                    {'kind': 'text', 'text': message}\n",
    "                ],\n",
    "                'messageId': uuid.uuid4().hex,\n",
    "            }\n",
    "    }\n",
    "\n",
    "    request = SendMessageRequest(\n",
    "        id=str(uuid.uuid4()),\n",
    "        params=MessageSendParams(**rpc_request)\n",
    "    )\n",
    "\n",
    "    response = await a2a_client.send_message(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeb0e4d",
   "metadata": {},
   "source": [
    "Just for fun, lets print out the response. We should expect to get back a response from our server in the form of a [JSONRPCResponse](https://a2aproject.github.io/A2A/latest/specification/#6112-jsonrpcresponse-object) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeef2219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root=SendMessageSuccessResponse(id='72144d5c-09ea-4716-ba9c-f4374dd2400d', jsonrpc='2.0', result=Task(artifacts=[Artifact(artifactId='168bc6c2-54e0-4a65-83ab-f0c50a3aa291', description=None, extensions=None, metadata=None, name='response', parts=[Part(root=TextPart(kind='text', metadata=None, text='Some of the trending topics in Artificial Intelligence right now, with a particular focus on anticipated developments in 2025, include the rise of AI agents, advancements in multimodal AI, and a continued emphasis on responsible AI and regulation.\\n\\nKey trending topics in AI include:\\n*   **AI Agents and Agentic AI** These are intelligent programs capable of acting autonomously to achieve specific goals, moving beyond simple chatbots to handle complex tasks, make decisions, and even collaborate independently. They are envisioned as the \"apps of the AI era,\" transforming business processes and simplifying daily life by performing tasks on behalf of users.\\n*   **Multimodal AI** This involves AI systems that can understand and process information from various sources, including text, images, video, and audio. This capability allows for a more comprehensive understanding of context and is expected to drive significant AI adoption.\\n*   **Enhanced AI Reasoning** A strong focus is on developing AI models with improved reasoning capabilities, allowing them to solve complex problems through logical steps, much like human thinking.\\n*   **Continued Generative AI (GenAI) Integration and Adoption** Generative AI, already a major trend, is seeing deeper integration into applications and increased adoption across workplaces to boost productivity. This includes the automation of repetitive tasks and the generation of content.\\n*   **Responsible AI, Ethics, and Regulation** As AI becomes more pervasive, there is a growing emphasis on ethical considerations, human oversight, and the development of comprehensive AI legislation and regulations, such as the EU AI Act, to ensure fundamental rights are respected.\\n*   **Small Language Models (SLMs)** These models are gaining prominence as more efficient and accessible alternatives to larger language models. They offer similar capabilities with significantly reduced computational resources and faster processing times.\\n*   **Embodied AI and Robotics** This emerging field aims to bring multimodal AI capabilities into the physical world, with increasing investment in advanced, generative AI-driven humanoid robotics.\\n*   **AI in Specific Sectors** AI\\'s influence is expanding rapidly in various sectors, including healthcare, scientific research, and the public sector, where it is being used to accelerate discoveries, improve patient outcomes, and enhance citizen experiences.\\n*   **Sustainability of AI** There\\'s a growing awareness of the substantial power consumption of cloud-based AI systems, leading to efforts to shift towards more sustainable and renewable energy sources for data centers.\\n*   **Privacy vs. Personalization** This remains a significant topic of discussion, with ongoing efforts to balance personalized AI experiences with safeguarding user privacy.\\n'))])], contextId='79b4c959-fffa-4d40-b69b-a0f66dab78b1', history=[Message(contextId='79b4c959-fffa-4d40-b69b-a0f66dab78b1', extensions=None, kind='message', messageId='ca170a14fd9747e8af672b01caf2bd35', metadata=None, parts=[Part(root=TextPart(kind='text', metadata=None, text='What are some trending topics in AI right now?'))], referenceTaskIds=None, role=<Role.user: 'user'>, taskId='cb609a5c-b01f-402d-83ec-07721e4dd216'), Message(contextId='79b4c959-fffa-4d40-b69b-a0f66dab78b1', extensions=None, kind='message', messageId='a5875471-88c4-4c2d-a23d-4fb41ef9e0f2', metadata=None, parts=[Part(root=TextPart(kind='text', metadata=None, text='Executing task...'))], referenceTaskIds=None, role=<Role.agent: 'agent'>, taskId='cb609a5c-b01f-402d-83ec-07721e4dd216')], id='cb609a5c-b01f-402d-83ec-07721e4dd216', kind='task', metadata=None, status=TaskStatus(message=None, state=<TaskState.completed: 'completed'>, timestamp='2025-07-10T00:02:18.186521+00:00')))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d863205",
   "metadata": {},
   "source": [
    "This is a bit tricky to parse through, since RPC calls come with a lot of attached metadata. Let's print just the json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9f100f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"8de65da8-7be0-47d2-a672-113ae76e4aa0\",\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"result\": {\n",
      "    \"artifacts\": [\n",
      "      {\n",
      "        \"artifactId\": \"eb3bfc6f-f025-44e0-9d84-da9bc25fcec5\",\n",
      "        \"name\": \"response\",\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"kind\": \"text\",\n",
      "            \"text\": \"Trending topics in Artificial Intelligence right now demonstrate a strong shift towards more capable, autonomous, and integrated AI systems, with a significant focus on practical applications across various sectors.\\n\\nKey trends include:\\n*   **AI Agents and Autonomous Systems** These are emerging as a major focus, with AI systems designed to proactively handle tasks, make decisions, and even take control of devices and execute complex operations independently. This signifies an evolution beyond simple chatbots towards more self-sufficient AI.\\n*   **Advanced Generative AI and Multimodality** Generative AI continues to evolve, moving past text-only applications to models that can process and integrate various data types like text, images, video, and audio. Upcoming models like OpenAI's GPT-5 are expected to unify advancements from multiple specialized models for greater versatility.\\n*   **Increased AI Adoption in the Workplace** AI tools are being increasingly integrated into daily workflows to automate repetitive tasks, boost productivity, and transform business operations across numerous industries.\\n*   **AI in Scientific Research and Healthcare** AI is playing a crucial role in accelerating scientific discovery, including the development of new materials, improving climate modeling, and accelerating the creation of life-saving drugs. In healthcare, AI is showing high accuracy in early disease detection and has the potential to revolutionize preventive care.\\n*   **Ethical AI and Regulation** As AI becomes more pervasive, there is a growing emphasis on ethical considerations, data privacy, and the need for broader regulations to ensure responsible AI development and deployment.\\n*   **Quantum Computing and AI Integration** The intersection of quantum computing and AI is poised to tackle currently unsolvable computational problems, with potential breakthroughs in areas like material science, climate modeling, and advanced cryptography.\\n*   **Explainable AI (XAI)** There's an increasing demand for AI models to be more transparent and interpretable, allowing users to understand how AI systems arrive at their decisions, fostering trust and alignment with human values.\\n*   **Reduced Inference Costs and Efficiency** AI models are becoming more practical and cost-effective to run, facilitating the deployment of complex multi-agent systems without prohibitive expenses.\\n*   **Embodied AI and Robotics** The goal of bringing AI's multimodal capabilities into the physical world is driving investments in advanced, generative AI-driven humanoid robotics.\\n*   **Retrieval-Augmented Generation (RAG)** This technique combines retrieval-based methods with generative AI, allowing models to access and generate information from vast external datasets, leading to more accurate and contextually relevant outputs.\\n\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"contextId\": \"9d009409-a78c-4bc7-9d15-69233b3d354e\",\n",
      "    \"history\": [\n",
      "      {\n",
      "        \"contextId\": \"9d009409-a78c-4bc7-9d15-69233b3d354e\",\n",
      "        \"kind\": \"message\",\n",
      "        \"messageId\": \"56d6ef452d624a9697282566a7d76aad\",\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"kind\": \"text\",\n",
      "            \"text\": \"What are some trending topics in AI right now?\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"user\",\n",
      "        \"taskId\": \"52d8dc78-fcdc-4e58-8613-1571ed53aac7\"\n",
      "      },\n",
      "      {\n",
      "        \"contextId\": \"9d009409-a78c-4bc7-9d15-69233b3d354e\",\n",
      "        \"kind\": \"message\",\n",
      "        \"messageId\": \"937affee-8071-47cc-9833-872afa48aa2d\",\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"kind\": \"text\",\n",
      "            \"text\": \"Executing task...\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"agent\",\n",
      "        \"taskId\": \"52d8dc78-fcdc-4e58-8613-1571ed53aac7\"\n",
      "      }\n",
      "    ],\n",
      "    \"id\": \"52d8dc78-fcdc-4e58-8613-1571ed53aac7\",\n",
      "    \"kind\": \"task\",\n",
      "    \"status\": {\n",
      "      \"state\": \"completed\",\n",
      "      \"timestamp\": \"2025-07-10T00:32:42.422769+00:00\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response_dict = response.model_dump(mode='json', exclude_none=True)\n",
    "print(json.dumps(response_dict, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5ab3b",
   "metadata": {},
   "source": [
    "If we want to extract just the text, which is what we will pass in as context to future interactions with our agent, we can do that as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "202cc085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trending topics in Artificial Intelligence right now demonstrate a strong shift towards more capable, autonomous, and integrated AI systems, with a significant focus on practical applications across various sectors.\n",
      "\n",
      "Key trends include:\n",
      "*   **AI Agents and Autonomous Systems** These are emerging as a major focus, with AI systems designed to proactively handle tasks, make decisions, and even take control of devices and execute complex operations independently. This signifies an evolution beyond simple chatbots towards more self-sufficient AI.\n",
      "*   **Advanced Generative AI and Multimodality** Generative AI continues to evolve, moving past text-only applications to models that can process and integrate various data types like text, images, video, and audio. Upcoming models like OpenAI's GPT-5 are expected to unify advancements from multiple specialized models for greater versatility.\n",
      "*   **Increased AI Adoption in the Workplace** AI tools are being increasingly integrated into daily workflows to automate repetitive tasks, boost productivity, and transform business operations across numerous industries.\n",
      "*   **AI in Scientific Research and Healthcare** AI is playing a crucial role in accelerating scientific discovery, including the development of new materials, improving climate modeling, and accelerating the creation of life-saving drugs. In healthcare, AI is showing high accuracy in early disease detection and has the potential to revolutionize preventive care.\n",
      "*   **Ethical AI and Regulation** As AI becomes more pervasive, there is a growing emphasis on ethical considerations, data privacy, and the need for broader regulations to ensure responsible AI development and deployment.\n",
      "*   **Quantum Computing and AI Integration** The intersection of quantum computing and AI is poised to tackle currently unsolvable computational problems, with potential breakthroughs in areas like material science, climate modeling, and advanced cryptography.\n",
      "*   **Explainable AI (XAI)** There's an increasing demand for AI models to be more transparent and interpretable, allowing users to understand how AI systems arrive at their decisions, fostering trust and alignment with human values.\n",
      "*   **Reduced Inference Costs and Efficiency** AI models are becoming more practical and cost-effective to run, facilitating the deployment of complex multi-agent systems without prohibitive expenses.\n",
      "*   **Embodied AI and Robotics** The goal of bringing AI's multimodal capabilities into the physical world is driving investments in advanced, generative AI-driven humanoid robotics.\n",
      "*   **Retrieval-Augmented Generation (RAG)** This technique combines retrieval-based methods with generative AI, allowing models to access and generate information from vast external datasets, leading to more accurate and contextually relevant outputs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'result' in response_dict and 'artifacts' in response_dict['result']:\n",
    "    artifacts = response_dict['result']['artifacts']\n",
    "    for artifact in artifacts:\n",
    "        if 'parts' in artifact:\n",
    "                for part in artifact['parts']:\n",
    "                    if 'text' in part:\n",
    "                        print(part['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015fe6c",
   "metadata": {},
   "source": [
    "...and that's it for now, we have covered a lot of ground, yet only brushed the surface of A2A and the power it truly holds.\n",
    "\n",
    "#### Summary: What have we learned?\n",
    "\n",
    "* Created a basic agent using Google's ADK\n",
    "* Covered the basics of A2A protocol\n",
    "* Launched a server to host our remote agent\n",
    "* RPC 2.0 Calls to interact with our agent\n",
    "\n",
    "#### Whats Next?\n",
    "In the coming weeks, we will be releasing a series of blog posts covering the nuances of A2A in depth:\n",
    "- RPC Deep Dive\n",
    "- The \"Agent\"\n",
    "- `AgentCard`, `AgentSkills`, `AgentExecutor`\n",
    "- Tools\n",
    "- Event Handling (tasks, messages...)\n",
    "- A2A with Model Context Protocol (MCP)\n",
    "- Debugging A2A + Tips and Tricks\n",
    "- Custom Agents\n",
    "- Authentication and Authorization\n",
    "\n",
    "\n",
    "We will also be building complex multi-agent workflows and learning how to orchestrate them with the power of [Littlehorse.io](https://littlehorse.io/)! Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596aa59f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
